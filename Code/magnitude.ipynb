{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-12 20:29:40.545038: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-06-12 20:29:43.479079: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import helpers\n",
    "from importlib import reload\n",
    "import random\n",
    "import models\n",
    "#import rocket\n",
    "import numpy as np\n",
    "import os\n",
    "from datetime import datetime\n",
    "from datetime import datetime, timezone"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load pressure drop list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            _DROP_        _LTST_           SOL   \\\n",
      "count  11560.000000  11560.000000  11560.000000   \n",
      "mean      -0.611105     12.289125    460.845329   \n",
      "min       -9.183000      8.001000     65.000000   \n",
      "25%       -0.603000     10.890500    309.000000   \n",
      "50%       -0.450000     12.193500    434.500000   \n",
      "75%       -0.389000     13.601250    639.000000   \n",
      "max       -0.345000     21.058000    861.000000   \n",
      "std        0.511879      1.839304    191.826052   \n",
      "\n",
      "            YYYY-MM-DDTHH:MM:SS.sss          RATIO  \n",
      "count                          11560  11560.000000  \n",
      "mean   2020-03-14 06:10:28.823963136      0.989166  \n",
      "min       2019-02-01 11:13:59.915000      0.502000  \n",
      "25%    2019-10-10 04:44:05.309999872      1.000000  \n",
      "50%    2020-02-16 02:58:21.841500160      1.000000  \n",
      "75%    2020-09-13 08:57:15.142499840      1.000000  \n",
      "max       2021-04-29 12:06:01.663000      1.000000  \n",
      "std                              NaN      0.044255  \n"
     ]
    }
   ],
   "source": [
    "reload(helpers)\n",
    "\n",
    "pressuredrops = pd.read_csv('allPdrops_ordered.csv').sort_values(by=' SOL ')\n",
    "\n",
    "unique_sols = pressuredrops[' SOL '].unique()\n",
    "missing_sols = []\n",
    "for sol in unique_sols:\n",
    "    file_path = f'seisdata_pq/sol_{sol}_seisdata.parquet'\n",
    "    if not os.path.isfile(file_path):\n",
    "        pressuredrops = pressuredrops[pressuredrops[' SOL '] != sol]\n",
    "        missing_sols.append(sol)\n",
    "\n",
    "pressuredrops[' YYYY-MM-DDTHH:MM:SS.sss '] = pressuredrops[' YYYY-MM-DDTHH:MM:SS.sss '].str.strip()\n",
    "pressuredrops[' YYYY-MM-DDTHH:MM:SS.sss '] = pd.to_datetime(pressuredrops[' YYYY-MM-DDTHH:MM:SS.sss '], format='%Y-%jT%H:%M:%S.%fZ')\n",
    "\n",
    "pressure_threshold = -0\n",
    "\n",
    "filtered_drops = pressuredrops[pressuredrops['_DROP_ '] < pressure_threshold]\n",
    "print(filtered_drops.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_list = [\n",
    "    #'Total',\n",
    "    #'Mid frequency data',\n",
    "    #'High frequency data'\n",
    "    'Time of day',\n",
    "    #'Time of year',\n",
    "    #'Time since last pDrop',\n",
    "    #'Drop width',\n",
    "    #'Max power',\n",
    "    #'Max/average',\n",
    "    'Gradient'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(helpers)\n",
    "window_size = 35\n",
    "features = 3\n",
    "power_threshold = 0\n",
    "samples_per_drop = 3\n",
    "pos_sample_spacing = 5\n",
    "sol_range = [14, 862]\n",
    "test_sols = helpers.generate_test_sols(sol_range=sol_range, percentage=0.0)\n",
    "all_df, pDrops_below_thresh, pressuredrops, pDrops_out_of_range, pos_thresh_dict = helpers.construct_pos_dataset(feature_list=feature_list, sol_range=sol_range, test_sols=test_sols, drop_list=filtered_drops, power_threshold=power_threshold, window_size = window_size, samples_per_drop = samples_per_drop, pos_sample_spacing= pos_sample_spacing, verbose=1, drop_magnitude=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concat features, create labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "\n",
    "rows_with_nan = all_df.isnull().any(axis=1)\n",
    "indices_with_nan = all_df.index[rows_with_nan]\n",
    "all_df.drop(indices_with_nan, inplace=True)\n",
    "all_df.columns = all_df.columns.astype(str)\n",
    "labels_df = all_df['Magnitude']\n",
    "features_df = all_df.drop(columns=['Magnitude'])\n",
    "\n",
    "\n",
    "assert len(features_df) == len(labels_df), f\"Mismatch in length of features and labels dataframes, features length {len(features_df)}, labels length {len(labels_df)}\"\n",
    "\n",
    "feature_scaler = StandardScaler()\n",
    "features_scaled = pd.DataFrame(feature_scaler.fit_transform(features_df), columns=features_df.columns, index=features_df.index)\n",
    "if 'Time of day' in feature_list:\n",
    "    mlst_series = features_scaled.index.to_series().apply(lambda x: helpers.datetime_to_mlst(x))\n",
    "    features_scaled[['sin_time', 'cos_time']] = mlst_series.apply(helpers.cyclical_encode_time).apply(pd.Series)\n",
    "if 'Time of year' in feature_list:\n",
    "    features_scaled[['sin_martian_time_of_year', 'cos_martian_time_of_year']] = features_scaled.index.to_series().apply(helpers.cyclical_encode_martian_time_of_year).apply(pd.Series)\n",
    "\n",
    "if 'Time since last pDrop' in feature_list:\n",
    "    pressuredrops_time = pressuredrops[[' YYYY-MM-DDTHH:MM:SS.sss ']].sort_values(' YYYY-MM-DDTHH:MM:SS.sss ')\n",
    "    features_scaled = features_scaled.sort_index()\n",
    "    features_scaled = pd.merge_asof(features_scaled, pressuredrops_time, left_index=True, right_on=' YYYY-MM-DDTHH:MM:SS.sss ', direction='backward')\n",
    "    features_scaled['time since last pDrop'] = features_scaled.index - features_scaled[' YYYY-MM-DDTHH:MM:SS.sss ']\n",
    "    features_scaled['time since last pDrop'] = features_scaled['time since last pDrop'].dt.total_seconds().astype(int)\n",
    "    features_scaled['time since last pDrop'] = features_scaled['time since last pDrop'].clip(upper=86400)\n",
    "    features_scaled.drop(' YYYY-MM-DDTHH:MM:SS.sss ', axis=1, inplace=True)\n",
    "\n",
    "X_train, X_eval, y_train, y_eval = train_test_split(features_scaled, labels_df, test_size=0.2, random_state=123)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_eval, y_eval, test_size=0.5, random_state=123)\n",
    "X_train_final = pd.concat([X_train, X_val])\n",
    "y_train_final = pd.concat([y_train, y_val])\n",
    "\n",
    "train_data = [X_train, y_train, X_val, y_val]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import r2_score\n",
    "reload(models)\n",
    "\n",
    "adjust_ratio = False\n",
    "\n",
    "best_xgb_clf = models.train_xgboost_regressor(data=[df.copy() for df in train_data], bayes_search=True, verbose=1, ratio=0, adjust_ratio=adjust_ratio)\n",
    "y_pred = best_xgb_clf.predict(X_test)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "print(\"Mean Absolute Error (MAE):\", mae)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(\"R-squared (RÂ²) Score:\", r2)\n",
    "\n",
    "best_xgb_clf.fit(X_train_final, y_train_final, eval_set=[(X_test, y_test)])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Model and Scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['magnitude_feature_scaler.joblib']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "joblib.dump(best_xgb_clf, 'magnitude_clf.joblib')\n",
    "joblib.dump(feature_scaler, 'magnitude_feature_scaler.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add Magnitudes to Classifier Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(helpers)\n",
    "classifier = 'magnitude_clf.joblib'\n",
    "feature_scaler = 'magnitude_feature_scaler.joblib'\n",
    "directory = 'xgboost_feature_selection/xgb_0_1.148e-08_Time of day_Time of year'\n",
    "feature_list = ['Time of Day', 'Gradient']\n",
    "helpers.add_magnitude_predictions(filtered_drops, directory, classifier, 35, feature_scaler, feature_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
